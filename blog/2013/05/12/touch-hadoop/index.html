
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>全分布式的Hadoop初体验 - Biaobiaoqi的博客</title>
  <meta name="author" content="Biaobiaoqi">

  
  <meta name="description" content="之前的时间里对Hadoop的使用都是基于学长所搭建起的实验环境的，没有完整的自己部署和维护过，最近抽时间初体验了在集群环境下装机、配置、运行的全过程，梳理总结到本文中。">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://biaobiaoqi.me/blog/2013/05/12/touch-hadoop/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Biaobiaoqi的博客" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<script type="text/javascript">

function addBlankTargetForLinks () {

  $('a[href^="http"]').each(function(){

      $(this).attr('target', '_blank');

  });

}

$(document).bind('DOMNodeInserted', function(event) {

  addBlankTargetForLinks();

});

</script>


<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F49481ec3305db999013860b0ccb3b16d' type='text/javascript'%3E%3C/script%3E"));
</script>


  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39900036-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Biaobiaoqi的博客</a></h1>
  
    <h2>做一个互联网世界的匠人</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:biaobiaoqi.me" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">首页</a></li>
  <li><a href="/blog/archives/">全部文章</a></li>
  <li><a href="/tag-cloud/">分类阅读</a></li>
  <li><a href="/resources/">其他推荐</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">全分布式的Hadoop初体验</h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-12T00:26:00+08:00" pubdate data-updated="true">May 12<span>th</span>, 2013</time>
        
         | <a href="#disqus_thread">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><h2>背景</h2>

<p>之前的时间里对 Hadoop 的使用都是基于学长所搭建起的实验环境的，没有完整的自己部署和维护过，最近抽时间初体验了在集群环境下装机、配置、运行的全过程，梳理总结到本文中。</p>

<h2>配置</h2>

<ul>
<li>内存:8G</li>
<li>CPU：i5-2400 3.1GHz；</li>
<li>硬盘：960G</li>
<li>系统：windows 7 旗舰 64bits</li>
</ul>


<!--more-->


<ul>
<li>虚拟机：VMware7.1.1</li>
<li>虚拟集群：</li>
<li><ul>
<li>T （master 节点）Ubuntu11.04 32 bits 内存 512MB；硬盘 100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T2（slave 节点） Ubuntu11.04 32 bits 内存 512MB；硬盘 100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T3（slave 节点） Ubuntu11.04 32 bits 内存 512MB；硬盘 100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T4（slave 节点） Ubuntu11.04 32 bits 内存 512MB；硬盘 100G；单核；</li>
</ul>
</li>
</ul>


<h2>环境准备</h2>

<h5>1.节点机器的配置</h5>

<p>配置固定 IP:修改<code>/etc/nerwork/interfaces</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>auto lo
</span><span class='line'>iface lo inet loopback
</span><span class='line'>address 192.168.108.131
</span><span class='line'>gateway 192.168.108.2
</span><span class='line'>netmask 192.168.108.0
</span><span class='line'>broadcast 192.168.108.0</span></code></pre></td></tr></table></div></figure>


<p>为了便于管理，建议按统一约定修改 hostname：修改<code>/etc/hostname</code>；同时，Hadoop 集群要求每个节点使用同一个账号来管理、运行，所以，也需要设置好公用账号。</p>

<h5>2.集群 ssh 配置</h5>

<p>ssh 相关原理和操作，参见博文<a href="http://biaobiaoqi.me/blog/2013/04/19/use-ssh/">《SSH 原理和使用》</a>。</p>

<p>在每台机器上生成密钥对，并将所有机器的公钥集成到 master 的<code>~/.ssh/authorized_keys</code>中,之后将这个文件分发到集群所有机器上。
这样，所有机器之间都可以实现免密码的 ssh 访问了。</p>

<p>使用如下指令，可以将本机的公钥添加到 master 的 authorized_keys 文件末尾。当所有节点都执行一遍以后，再将 master 的 authorized_keys 发布到各个节点上。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#cat .ssh/id_rsa.pub | ssh T 'cat &gt;&gt; ~/.ssh/authorized_keys'</span></code></pre></td></tr></table></div></figure>


<h5>3.工具脚本</h5>

<p>在分布式的环境里，运维工作的自动化很有必要。为了方便集群的运维，我写了两个简单的 batch 脚本。</p>

<h6>统一执行脚本</h6>

<p>在所有节点上执行同样的动作。使用时，在 master 节点上调用 batch 脚本，参数为对应的 batch 执行语句。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>#Program:
</span><span class='line'># Execute instructions in hosts in slaveslist.
</span><span class='line'>#Description:
</span><span class='line'>#2013/5/8 biaobiaoqi First Release
</span><span class='line'>if [ $# -lt 1 ]; then
</span><span class='line'>   echo  "usage: $0 COMMAND"
</span><span class='line'>   exit 0
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>for i in `cat slaveslist`
</span><span class='line'>do
</span><span class='line'>   ssh biaobiaoqi@$i "$1"
</span><span class='line'>done
</span></code></pre></td></tr></table></div></figure>


<p>脚本中使用的 slaveslist 文件保存着所有 slave 节点的 hostname，需要与脚本放在同一个工作目录下。</p>

<h6>统一替部署脚本</h6>

<p>将主节点的某文件或目录统一的更新部署替换到所有节点上（注意，所有节点拥有相同的目录结构，即替换的文件路径相同）。</p>

<p>遇到 hadoop 集群中节点的增删改动需要修改配置文件的，都可以通过这个脚本便捷的部署。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>#Program:
</span><span class='line'>#    Put the dirctory into all nodes of the cluster as the same path.
</span><span class='line'>#Description:
</span><span class='line'>#2013/5/10     biaobiaoqi     First Release
</span><span class='line'>if [ $# -lt 1 ]; then
</span><span class='line'>   echo "Usage $0 DIR_PATH"
</span><span class='line'>   exit 0
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>for i in `cat slaveslist`
</span><span class='line'>do
</span><span class='line'>   ssh $i "rm ~/tmp -rf"
</span><span class='line'>   scp -r $1 $i:~/tmp
</span><span class='line'>   ssh $i "rm -rf $1;  mv ~/tmp $1"
</span><span class='line'>done
</span></code></pre></td></tr></table></div></figure>


<h5>4.配置 hosts 文件</h5>

<p>由于 hadoop 体系在处理节点时，是使用的 hostname，而非 IP，所以必须先配置好 hostname 和 IP 的关系。
在一台机器上修改<code>/etc/hosts</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#/etc/hosts
</span><span class='line'>127.0.0.1     localhost
</span><span class='line'>192.168.108.128     T3
</span><span class='line'>192.168.108.129     T2
</span><span class='line'>192.168.108.130 T
</span><span class='line'>192.168.108.131 T4</span></code></pre></td></tr></table></div></figure>


<p>然后使用统一执行脚本，将它发布到所有节点上。</p>

<p>值得注意的是，在<code>/etc/hostsname</code>中修改了 host name 之后，如果不同步的修改<code>/etc/hosts</code>中的相关信息，则在 sudo 操作时出现 <code>sudo: unable to resolve host</code>  的提示。原因是机器无法解析主机名。</p>

<p>修改<code>/etc/hosts</code>时也要特别注意，如果改成<code>127.0.0.1 localhost HOSTNAME</code> (其中 HOSTNAME 是主机名)的形式，在开启 hadoop 集群时，会出现 datanode 无法正常访问 namenode，算是个小 bug 吧。所以得把 hosts 文件写成如上的形式。</p>

<h5>5.配置 Java 环境</h5>

<p>Hadoop 需要 Java1.6 或更高版本，记住 Java 的安装目录，之后需要在 hadoop 配置过程中用到。</p>

<h2>安装 Hadoop</h2>

<h5>1.下载 Hadoop</h5>

<p>从官网下载 <a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Hadoop 发布版</a>（博主使用的是较早的稳定版 0.20.2）</p>

<p>关于版本选择，推荐阅读：<a href="http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/">Hadoop 版本选择探讨</a></p>

<h5>2.部署</h5>

<p>解压下载好的 Hadoop，后放到合适的目录下。这里假定放置在/home/USER/ 的目录下</p>

<p>在<code>/home/USER/.bashrc</code>(其中 USER 为集群的用户名)文件中，增加如下语句，设定 Hadoop 相关的路径信息：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export JAVA_HOME=/usr/lib/jvm/java-6-openjdk
</span><span class='line'>export HADOOP_HOME=/home/hadoop/Hadoop
</span><span class='line'>export HADOOP_CONF=$HADOOP_HOME/conf
</span><span class='line'>export HADOOP_PATH=$HADOOP_HOME/bin
</span><span class='line'>export PATH=$HADOOP_PATH:$PATH
</span><span class='line'>export CLASSPATH=.:$JAVA_HOME/bin:$PATH:$HADOOP_HOME:$HADOOP_HOME/bin</span></code></pre></td></tr></table></div></figure>


<h6>Hadoop 核心配置修改</h6>

<p>配置文件在<code>$HADOOP_HOME/conf</code>目录下，其中基础配置比较重要的有三个：core-site.xml, hdfs-site.xml, mapred-site.xml。（当然，每个配置文件都有其细节作用，不过在初步实践 hadoop 时，理解这三个配置文件中的几个重要配置项就够了）</p>

<p>一般的，有三种可选模式。即本地模式、伪分布式模式和全分布式模式。前两种只是在单机环境下，后一种才是生产环境下的常用方式。《Hadoop 权威指南》和《Hadoop 实战》等书中都有讲到不同方式的配置，这里博主仅描述实验环境下 4 节点的全分布式配置。</p>

<p>core-site.xml 整个 hadoop 的顶层配置</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt; 
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>         &lt;value&gt;/home/biaobiaoqi/UDMS/hadoop-data/tmp-base&lt;/value&gt;
</span><span class='line'>         &lt;description&gt;
</span><span class='line'>        存放临时目录的路径，默认也被用来存储 hdfs 的元数据和文件数据，值得注意的是，hadoop 账户对所设定的本地路径是否有足够的操作权限。之后再 hdfs-site.xml 中设定的 dfs.data.dir 和 dfs.name.dir 也要注意同样的问题
</span><span class='line'>        &lt;/description&gt; 
</span><span class='line'>    &lt;/property&gt; 
</span><span class='line'>
</span><span class='line'>     &lt;property&gt;   
</span><span class='line'>          &lt;name&gt;fs.default.name&lt;/name&gt; 
</span><span class='line'>          &lt;value&gt;hdfs://T:9000/&lt;/value&gt;
</span><span class='line'>          &lt;description&gt;
</span><span class='line'>        默认文件系统的标记。这个 URI 标记了文件系统的实现方式。UIR 的协议决定了文件系统的实现类，而后面的值决定了文件系统的地址、端口等信息。
</span><span class='line'>        &lt;/description&gt;
</span><span class='line'>     &lt;/property&gt; 
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span></code></pre></td></tr></table></div></figure>


<p>hdfs-site.xml 存储 HDFS 相关的信息</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt; 
</span><span class='line'>    &lt;property&gt;   
</span><span class='line'>          &lt;name&gt;dfs.replication&lt;/name&gt;   
</span><span class='line'>          &lt;value&gt;3&lt;/value&gt;   
</span><span class='line'>          &lt;description&gt;默认的块的副本数量。实际的副本数量可以在文件写入的时候确定，默认的副本数则是在没有指定写入副本时被使用。 &lt;/description&gt; 
</span><span class='line'>     &lt;/property&gt;
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>         &lt;name&gt;dfs.name.dir&lt;/name&gt;
</span><span class='line'>          &lt;value&gt;/home/hadoop/hadoop-data/meta-data&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;
</span><span class='line'>        设定 hdfs 的元数据信息存储地址。在 namenode 上。
</span><span class='line'>        &lt;/description&gt;
</span><span class='line'>     &lt;/property&gt;
</span><span class='line'>     &lt;property&gt;
</span><span class='line'>          &lt;name&gt;dfs.data.dir&lt;/name&gt;
</span><span class='line'>          &lt;value&gt;/home/hadoop/hadoop-data/data&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;
</span><span class='line'>        设定 hdfs 的数据存储地址。在 datanode 上。
</span><span class='line'>        &lt;/description&gt;
</span><span class='line'>     &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span></code></pre></td></tr></table></div></figure>


<p>mapred-site.xml 存储 mapreduce 作业相关配置</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt; 
</span><span class='line'>    &lt;property&gt;   
</span><span class='line'>          &lt;name&gt;mapred.job.tracker&lt;/name&gt;
</span><span class='line'>          &lt;value&gt;T:9001&lt;/value&gt;   
</span><span class='line'>          &lt;description&gt; Mapreduce 的 job tracker 所在的节点和端口。&lt;/description&gt; 
</span><span class='line'>     &lt;/property&gt;
</span><span class='line'>&lt;/configuration&gt;
</span></code></pre></td></tr></table></div></figure>


<p>hosts 文件存储了 master 节点</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>T</span></code></pre></td></tr></table></div></figure>


<p>slaves 文件存储着所有的 slaves 节点</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>T2
</span><span class='line'>T3
</span><span class='line'>T4</span></code></pre></td></tr></table></div></figure>


<h2>启动集群</h2>

<h5>1.格式化 namenode</h5>

<p>如果是第一次起动集群，需要先格式化 HDFS。</p>

<p>namenode 存放了 HDFS 的元数据，故可以看成是对 HDFS 的格式化。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$HADOOP_HOME/bin/hadoop namenode -format</span></code></pre></td></tr></table></div></figure>


<h5>2.启动守护进程</h5>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$HADOOP_HOME/bin/start-all.sh </span></code></pre></td></tr></table></div></figure>


<p>等价于如下命令执行：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># start dfs daemons
</span><span class='line'>$"$bin"/start-dfs.sh --config $HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># start mapred daemons
</span><span class='line'>$"$bin"/start-mapred.sh --config $HADOOP_CONF_DIR
</span></code></pre></td></tr></table></div></figure>


<p>如果成功，打开 http://T:50070 (T 为集群 master 节点)，可以看到 HDFS 的运行情况，包括节点数量、空间大小等。这是 Hadoop 自带的 HDFS 监控页面；同样的，http://T:50030 是 Mapreduce 的监控界面。</p>

<p>如果没有成功，根据$HADOOP_HOME/logs 目录下的日志文件信息 debug。</p>

<h5>3.常见问题</h5>

<ul>
<li>namenode 无法启动：</li>
<li><ul>
<li>删除掉本地文件系统中 HDFS 的目录文件，重新格式化 HDFS。</li>
</ul>
</li>
<li><ul>
<li>HDFS 目录的权限不够，更改权限设置等。</li>
</ul>
</li>
<li>namenode 启动成功，datanode 无法连接：检查 hosts 文件是否设置正确；检查各个配置文件中地址值是否使用了 IP 而不是 hostname。</li>
<li>namenode 启动成功，datanode 无法启动：Incompatible namespaceIDs，由于频繁格式化，造成 dfs.name.dir/current/VERSION 与 dfs.data.dir/current/VERSION 数据不一致。</li>
<li>SafeModeException： 分布式系统启动时，会进入安全模式，安全模式下，hadoop 是无法执行的。一般的等待一会儿，就可以正常使用了。如果是由于之前集群崩溃造成的无法自动退出安全模式的情况，则需要如下特殊处理了</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$/$HADOOP_HOME/bin/hadoop dfsadmin -safemode leave </span></code></pre></td></tr></table></div></figure>


<h2>初体验</h2>

<p>最简单的尝试就是使用 Hadoop 自带的 wordcount 程序了，参照<a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2504205.html">这篇文章</a>，描述很详细。</p>

<p>其他的一些尝试： <a href="http://www.cnblogs.com/rilley/archive/2012/02/13/2349858.html">动态增删节点</a> 、 <a href="http://www.cnblogs.com/ggjucheng/archive/2012/04/18/2454696.html">修改备份数量</a></p>

<h2>参考</h2>

<p><a href="http://hadoop.apache.org/docs/stable/cluster_setup.html">offical document: Cluster Setup</a></p>

<p class='post-footer'>&nbsp;原文地址：<a href='http://biaobiaoqi.me/blog/2013/05/12/touch-hadoop/'>http://biaobiaoqi.me/blog/2013/05/12/touch-hadoop/</a><br/>&nbsp;版权声明：自由转载-非商用-非衍生-保持署名| <a href='http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh'>Creative Commons BY-NC-ND 3.0</a></p>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Biaobiaoqi</span></span>

      








  


<time datetime="2013-05-12T00:26:00+08:00" pubdate data-updated="true">May 12<span>th</span>, 2013</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/tech/'>tech</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  
  
    <!-- JiaThis Button BEGIN -->
<div class="jiathis_style_32x32">
<a class="jiathis_button_tsina"></a>
<a class="jiathis_button_weixin"></a>
<a class="jiathis_button_renren"></a>
<a class="jiathis_button_douban"></a>
<a class="jiathis_button_googleplus"></a>
<a href="http://www.jiathis.com/share?uid=1814751" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
var jiathis_config={
	data_track_clickback:true,
	summary:"",
	shortUrl:true,
	hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1814751" charset="utf-8"></script>
<!-- JiaThis Button END -->


  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2013/05/09/networking-in-virtual-machine/" title="Previous Post: 虚拟机中的网络配置">&laquo; 虚拟机中的网络配置</a>
      
      
        <a class="basic-alignment right" href="/blog/2013/05/12/mbti-test/" title="Next Post: 我的MBTI职业性格测试">我的MBTI职业性格测试 &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <!--h1>Comments</h1-->
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>标签云</h1>
 <br /> 
  <ul class="tag-cloud">
    <a style="font-size: 113%" href="/tags/hadoop/">Hadoop</a> &nbsp 
<a style="font-size: 139%" href="/tags/jvm/">JVM</a> &nbsp 
<a style="font-size: 145%" href="/tags/java/">Java</a> &nbsp 
<a style="font-size: 143%" href="/tags/pat/">PAT</a> &nbsp 
<a style="font-size: 107%" href="/tags/octopress/">octopress</a> &nbsp 
<a style="font-size: 137%" href="/tags/fen-bu-shi/">分布式</a> &nbsp 
<a style="font-size: 135%" href="/tags/zi-jie-ma/">字节码</a> &nbsp 
<a style="font-size: 135%" href="/tags/si-kao/">思考</a> &nbsp 
<a style="font-size: 180%" href="/tags/ji-zhu/">技术</a> &nbsp 
<a style="font-size: 107%" href="/tags/cao-zuo-xi-tong/">操作系统</a> &nbsp 
<a style="font-size: 126%" href="/tags/fang-fa-lun/">方法论</a> &nbsp 
<a style="font-size: 129%" href="/tags/sheng-huo/">生活</a> &nbsp 
<a style="font-size: 148%" href="/tags/suan-fa/">算法</a> &nbsp 
<a style="font-size: 118%" href="/tags/wang-luo/">网络</a> &nbsp 
<a style="font-size: 135%" href="/tags/du-shu/">读书</a>

  </ul>
</section>
<section>
  <h1>关于</h1>
   <p>biaobiaoqi是我的网络id。<br><br>
   我的博客日志：<a href="http://biaobiaoqi.me/blog/2013/09/01/blog-log/">请戳</a><br><br>
   更多信息请戳：<a href="http://about.me/biaobiaoqi">about.me</a><br>
	</p>
</section>




  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Biaobiaoqi -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  - <span class="credit">Theme by <a href="http://www.gehaxelt.in">Gehaxelt</a></span>
  <span class="credit">and <a href="http://www.it-solutions-neef.de">IT Solutions Neef</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'biaobiaoqisblog';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://biaobiaoqi.me/blog/2013/05/12/touch-hadoop/';
        var disqus_url = 'http://biaobiaoqi.me/blog/2013/05/12/touch-hadoop/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
